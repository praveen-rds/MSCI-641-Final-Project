{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9012858,"sourceType":"datasetVersion","datasetId":5430468},{"sourceId":9082595,"sourceType":"datasetVersion","datasetId":5479889},{"sourceId":9096850,"sourceType":"datasetVersion","datasetId":5490001}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_scheduler, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-03T23:33:54.277675Z","iopub.execute_input":"2024-08-03T23:33:54.278891Z","iopub.status.idle":"2024-08-03T23:33:54.284215Z","shell.execute_reply.started":"2024-08-03T23:33:54.278854Z","shell.execute_reply":"2024-08-03T23:33:54.282986Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Load your data\ntrain = pd.read_csv('/kaggle/input/datasetscsv/train_data.csv')\nval = pd.read_csv('/kaggle/input/datasetscsv/val_data.csv')\ntest = pd.read_csv('/kaggle/input/datasetscsv/test.csv')\n\n# Ensure the columns are named correctly\ntrain_inputs = train['text'].astype(str).tolist()\ntrain_labels = train['labels'].astype(str).tolist()\n\nval_inputs = val['text'].astype(str).tolist()\nval_labels = val['labels'].astype(str).tolist()\n\ntest_inputs = test['text'].astype(str).tolist()\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ntrain_labels = label_encoder.fit_transform([label.strip(\"[]'\") for label in train_labels])\nval_labels = label_encoder.transform([label.strip(\"[]'\") for label in val_labels])\n\n# Save label mapping for later use\nlabel_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(\"Label Mapping:\", label_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T23:31:59.716067Z","iopub.execute_input":"2024-08-03T23:31:59.716417Z","iopub.status.idle":"2024-08-03T23:31:59.861106Z","shell.execute_reply.started":"2024-08-03T23:31:59.716389Z","shell.execute_reply":"2024-08-03T23:31:59.860210Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Label Mapping: {'multi': 0, 'passage': 1, 'phrase': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\n\ntrain_encodings = tokenizer(train_inputs, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_inputs, truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_inputs, truncation=True, padding=True, max_length=512)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T23:32:03.687830Z","iopub.execute_input":"2024-08-03T23:32:03.688208Z","iopub.status.idle":"2024-08-03T23:32:29.269461Z","shell.execute_reply.started":"2024-08-03T23:32:03.688179Z","shell.execute_reply":"2024-08-03T23:32:29.268445Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = CustomDataset(train_encodings, train_labels)\nval_dataset = CustomDataset(val_encodings, val_labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T23:33:00.146226Z","iopub.execute_input":"2024-08-03T23:33:00.146609Z","iopub.status.idle":"2024-08-03T23:33:00.154058Z","shell.execute_reply.started":"2024-08-03T23:33:00.146580Z","shell.execute_reply":"2024-08-03T23:33:00.153146Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T23:33:02.748622Z","iopub.execute_input":"2024-08-03T23:33:02.748981Z","iopub.status.idle":"2024-08-03T23:33:02.769182Z","shell.execute_reply.started":"2024-08-03T23:33:02.748952Z","shell.execute_reply":"2024-08-03T23:33:02.768218Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_labels)), hidden_dropout_prob=0.2)\n\noptimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n\nnum_epochs = 20\nnum_training_steps = num_epochs * len(train_loader)\n\nnum_warmup_steps = int(0.1 * num_training_steps)  # 10% of training steps for warmup\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T23:34:01.450321Z","iopub.execute_input":"2024-08-03T23:34:01.451063Z","iopub.status.idle":"2024-08-03T23:34:02.032406Z","shell.execute_reply.started":"2024-08-03T23:34:01.451034Z","shell.execute_reply":"2024-08-03T23:34:02.031482Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2024-08-03T23:34:08.176102Z","iopub.execute_input":"2024-08-03T23:34:08.176855Z","iopub.status.idle":"2024-08-03T23:34:08.181800Z","shell.execute_reply.started":"2024-08-03T23:34:08.176824Z","shell.execute_reply":"2024-08-03T23:34:08.180859Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        optimizer.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        lr_scheduler.step()\n\n    print(f\"Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T23:34:17.908353Z","iopub.execute_input":"2024-08-03T23:34:17.908959Z","iopub.status.idle":"2024-08-04T01:24:58.749444Z","shell.execute_reply.started":"2024-08-03T23:34:17.908926Z","shell.execute_reply":"2024-08-04T01:24:58.748229Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch: 1/20, Loss: 0.7959491014480591\nEpoch: 2/20, Loss: 1.1583460569381714\nEpoch: 3/20, Loss: 1.1604667901992798\nEpoch: 4/20, Loss: 0.7535871863365173\nEpoch: 5/20, Loss: 0.29121577739715576\nEpoch: 6/20, Loss: 0.4476976990699768\nEpoch: 7/20, Loss: 0.34571388363838196\nEpoch: 8/20, Loss: 0.7011649012565613\nEpoch: 9/20, Loss: 0.5663749575614929\nEpoch: 10/20, Loss: 0.025963740423321724\nEpoch: 11/20, Loss: 0.009006178937852383\nEpoch: 12/20, Loss: 0.0018188500544056296\nEpoch: 13/20, Loss: 0.7061797976493835\nEpoch: 14/20, Loss: 0.0005079236580058932\nEpoch: 15/20, Loss: 1.0225307941436768\nEpoch: 16/20, Loss: 0.8170844316482544\nEpoch: 17/20, Loss: 0.00045374358887784183\nEpoch: 18/20, Loss: 0.0010210965992882848\nEpoch: 19/20, Loss: 0.0001452578726457432\nEpoch: 20/20, Loss: 0.00026584244915284216\n","output_type":"stream"}]},{"cell_type":"code","source":"# Switch to evaluation mode\nmodel.eval()\n\n# Prepare to collect predictions and references\npredictions = []\nreferences = []\n\n# Perform evaluation\nwith torch.no_grad():\n    for batch in val_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        preds = torch.argmax(outputs.logits, dim=-1)\n        predictions.extend(preds.cpu().numpy())\n        references.extend(batch['labels'].cpu().numpy())\n\n# Decode integer labels back to original label strings\ndecoded_predictions = label_encoder.inverse_transform(predictions)\ndecoded_references = label_encoder.inverse_transform(references)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(references, predictions)\nf1 = f1_score(references, predictions, average='weighted')\nreport = classification_report(references, predictions, target_names=label_encoder.classes_)\n\nprint(f\"Validation Accuracy: {accuracy}\")\nprint(f\"Validation F1 Score: {f1}\")\nprint(\"Classification Report:\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T01:26:55.601252Z","iopub.execute_input":"2024-08-04T01:26:55.602004Z","iopub.status.idle":"2024-08-04T01:27:09.002028Z","shell.execute_reply.started":"2024-08-04T01:26:55.601973Z","shell.execute_reply":"2024-08-04T01:27:09.001043Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.725\nValidation F1 Score: 0.7241485371342836\nClassification Report:\n              precision    recall  f1-score   support\n\n       multi       0.77      0.65      0.71        84\n     passage       0.73      0.69      0.71       154\n      phrase       0.70      0.79      0.74       162\n\n    accuracy                           0.73       400\n   macro avg       0.74      0.71      0.72       400\nweighted avg       0.73      0.72      0.72       400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/roberta_clickbait_model\")\ntokenizer.save_pretrained(\"/kaggle/working/roberta_clickbait_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T01:42:01.411090Z","iopub.execute_input":"2024-08-04T01:42:01.411496Z","iopub.status.idle":"2024-08-04T01:42:02.673433Z","shell.execute_reply.started":"2024-08-04T01:42:01.411466Z","shell.execute_reply":"2024-08-04T01:42:02.672519Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/roberta_clickbait_tokenizer/tokenizer_config.json',\n '/kaggle/working/roberta_clickbait_tokenizer/special_tokens_map.json',\n '/kaggle/working/roberta_clickbait_tokenizer/vocab.json',\n '/kaggle/working/roberta_clickbait_tokenizer/merges.txt',\n '/kaggle/working/roberta_clickbait_tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Load test data\ntest_inputs = test['text'].astype(str).tolist()\n\n# Tokenize test data\ntest_encodings = tokenizer(test_inputs, truncation=True, padding=True, max_length=512)\n\nclass TestDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n# Create DataLoader for test data\ntest_dataset = TestDataset(test_encodings)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Switch to evaluation mode\nmodel.eval()\n\n# Prepare to collect predictions\npredictions = []\n\n# Perform prediction\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        preds = torch.argmax(outputs.logits, dim=-1)\n        predictions.extend(preds.cpu().numpy())\n\n# Decode integer labels back to original label strings\ndecoded_predictions = label_encoder.inverse_transform(predictions)\n\n# Prepare the output DataFrame\npred_df = pd.DataFrame({\n    'id': test['id'],  # Adjust based on your actual test data format\n    'spoilerType': decoded_predictions\n})\n\n# Save predictions to CSV\npred_df.to_csv('/kaggle/working/roberta_predictions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T01:27:24.350467Z","iopub.execute_input":"2024-08-04T01:27:24.350868Z","iopub.status.idle":"2024-08-04T01:27:40.587211Z","shell.execute_reply.started":"2024-08-04T01:27:24.350835Z","shell.execute_reply":"2024-08-04T01:27:40.586207Z"},"trusted":true},"execution_count":44,"outputs":[]}]}