{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9012858,"sourceType":"datasetVersion","datasetId":5430468},{"sourceId":9082595,"sourceType":"datasetVersion","datasetId":5479889},{"sourceId":9096850,"sourceType":"datasetVersion","datasetId":5490001}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import DebertaTokenizer, DebertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-04T18:45:04.110539Z","iopub.execute_input":"2024-08-04T18:45:04.111256Z","iopub.status.idle":"2024-08-04T18:45:10.672373Z","shell.execute_reply.started":"2024-08-04T18:45:04.111224Z","shell.execute_reply":"2024-08-04T18:45:10.671367Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load your data\ntrain = pd.read_csv('/kaggle/input/datasetscsv/train_data.csv')\nval = pd.read_csv('/kaggle/input/datasetscsv/val_data.csv')\ntest = pd.read_csv('/kaggle/input/datasetscsv/test.csv')\n\n# Ensure the columns are named correctly\ntrain_inputs = train['text'].astype(str).tolist()\ntrain_labels = train['labels'].astype(str).tolist()\n\nval_inputs = val['text'].astype(str).tolist()\nval_labels = val['labels'].astype(str).tolist()\n\ntest_inputs = test['text'].astype(str).tolist()\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ntrain_labels = label_encoder.fit_transform([label.strip(\"[]'\") for label in train_labels])\nval_labels = label_encoder.transform([label.strip(\"[]'\") for label in val_labels])\n\n# Save label mapping for later use\nlabel_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(\"Label Mapping:\", label_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:45:12.388844Z","iopub.execute_input":"2024-08-04T18:45:12.389378Z","iopub.status.idle":"2024-08-04T18:45:12.694498Z","shell.execute_reply.started":"2024-08-04T18:45:12.389349Z","shell.execute_reply":"2024-08-04T18:45:12.693586Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Label Mapping: {'multi': 0, 'passage': 1, 'phrase': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 'microsoft/deberta-base'\ntokenizer = DebertaTokenizer.from_pretrained(model_name)\n\ntrain_encodings = tokenizer(train_inputs, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_inputs, truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_inputs, truncation=True, padding=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:45:13.740680Z","iopub.execute_input":"2024-08-04T18:45:13.741488Z","iopub.status.idle":"2024-08-04T18:45:40.186822Z","shell.execute_reply.started":"2024-08-04T18:45:13.741456Z","shell.execute_reply":"2024-08-04T18:45:40.186024Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56527b9d92444b7e9493db034571310a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b438b2eeaf45efb256df33aa1d45f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"301e561515f541898a46bfcf4166e023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa1de5d8fa814fa8a46124f1a880ed7d"}},"metadata":{}}]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = CustomDataset(train_encodings, train_labels)\nval_dataset = CustomDataset(val_encodings, val_labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:45:44.471494Z","iopub.execute_input":"2024-08-04T18:45:44.472218Z","iopub.status.idle":"2024-08-04T18:45:44.480899Z","shell.execute_reply.started":"2024-08-04T18:45:44.472184Z","shell.execute_reply":"2024-08-04T18:45:44.479415Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:46:27.188413Z","iopub.execute_input":"2024-08-04T18:46:27.189150Z","iopub.status.idle":"2024-08-04T18:46:27.193898Z","shell.execute_reply.started":"2024-08-04T18:46:27.189119Z","shell.execute_reply":"2024-08-04T18:46:27.192860Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = DebertaForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_labels)))\n\noptimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n\nnum_epochs = 10\nnum_training_steps = num_epochs * len(train_loader)\n\nnum_warmup_steps = int(0.1 * num_training_steps)  # 10% of training steps for warmup\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nprint(f'Using device: {device}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:46:29.129718Z","iopub.execute_input":"2024-08-04T18:46:29.130225Z","iopub.status.idle":"2024-08-04T18:46:34.503014Z","shell.execute_reply.started":"2024-08-04T18:46:29.130189Z","shell.execute_reply":"2024-08-04T18:46:34.502108Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d87ecd8d1845538086d16ecdb89d66"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        optimizer.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        lr_scheduler.step()\n\n    print(f\"Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:46:36.432861Z","iopub.execute_input":"2024-08-04T18:46:36.433396Z","iopub.status.idle":"2024-08-04T20:12:10.742084Z","shell.execute_reply.started":"2024-08-04T18:46:36.433364Z","shell.execute_reply":"2024-08-04T20:12:10.740772Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch: 1/20, Loss: 1.0868505239486694\nEpoch: 2/20, Loss: 0.6080507636070251\nEpoch: 3/20, Loss: 0.4876149892807007\nEpoch: 4/20, Loss: 0.8239059448242188\nEpoch: 5/20, Loss: 0.028545040637254715\nEpoch: 6/20, Loss: 0.5291695594787598\nEpoch: 7/20, Loss: 0.0013126988196745515\nEpoch: 8/20, Loss: 0.00021257127809803933\nEpoch: 9/20, Loss: 0.00022988552518654615\nEpoch: 10/20, Loss: 0.000445537269115448\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:69\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m clip_coef \u001b[38;5;241m=\u001b[39m \u001b[43mmax_norm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_norm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# when the gradients do not reside in CPU memory.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m clip_coef_clamped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(clip_coef, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:913\u001b[0m, in \u001b[0;36mTensor.__rdiv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rdiv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreciprocal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m other\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Switch to evaluation mode\nmodel.eval()\n\n# Prepare to collect predictions and references\npredictions = []\nreferences = []\n\n# Perform evaluation\nwith torch.no_grad():\n    for batch in val_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        preds = torch.argmax(outputs.logits, dim=-1)\n        predictions.extend(preds.cpu().numpy())\n        references.extend(batch['labels'].cpu().numpy())\n\n# Decode integer labels back to original label strings\ndecoded_predictions = label_encoder.inverse_transform(predictions)\ndecoded_references = label_encoder.inverse_transform(references)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(references, predictions)\nf1 = f1_score(references, predictions, average='weighted')\nreport = classification_report(references, predictions, target_names=label_encoder.classes_)\n\nprint(f\"Validation Accuracy: {accuracy}\")\nprint(f\"Validation F1 Score: {f1}\")\nprint(\"Classification Report:\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T20:13:30.357390Z","iopub.execute_input":"2024-08-04T20:13:30.357790Z","iopub.status.idle":"2024-08-04T20:13:50.575997Z","shell.execute_reply.started":"2024-08-04T20:13:30.357761Z","shell.execute_reply":"2024-08-04T20:13:50.574999Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.7175\nValidation F1 Score: 0.7153727398901079\nClassification Report:\n              precision    recall  f1-score   support\n\n       multi       0.78      0.61      0.68        84\n     passage       0.72      0.68      0.70       154\n      phrase       0.69      0.81      0.75       162\n\n    accuracy                           0.72       400\n   macro avg       0.73      0.70      0.71       400\nweighted avg       0.72      0.72      0.72       400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load test data\ntest_inputs = test['text'].astype(str).tolist()\n\n# Tokenize test data\ntest_encodings = tokenizer(test_inputs, truncation=True, padding=True, max_length=512)\n\nclass TestDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n# Create DataLoader for test data\ntest_dataset = TestDataset(test_encodings)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Switch to evaluation mode\nmodel.eval()\n\n# Prepare to collect predictions\npredictions = []\n\n# Perform prediction\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        preds = torch.argmax(outputs.logits, dim=-1)\n        predictions.extend(preds.cpu().numpy())\n\n# Decode integer labels back to original label strings\ndecoded_predictions = label_encoder.inverse_transform(predictions)\n\n# Prepare the output DataFrame\npred_df = pd.DataFrame({\n    'id': test['id'],  # Adjust based on your actual test data format\n    'spoilerType': decoded_predictions\n})\n\n# Save predictions to CSV\npred_df.to_csv('/kaggle/working/deberta_predictions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T20:14:14.692862Z","iopub.execute_input":"2024-08-04T20:14:14.693641Z","iopub.status.idle":"2024-08-04T20:14:36.988775Z","shell.execute_reply.started":"2024-08-04T20:14:14.693610Z","shell.execute_reply":"2024-08-04T20:14:36.988010Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Save the trained model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/deberta_clickbait_model\")\ntokenizer.save_pretrained(\"/kaggle/working/deberta_clickbait_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T20:20:21.552141Z","iopub.execute_input":"2024-08-04T20:20:21.552889Z","iopub.status.idle":"2024-08-04T20:20:23.015041Z","shell.execute_reply.started":"2024-08-04T20:20:21.552857Z","shell.execute_reply":"2024-08-04T20:20:23.014162Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/deberta_clickbait_tokenizer/tokenizer_config.json',\n '/kaggle/working/deberta_clickbait_tokenizer/special_tokens_map.json',\n '/kaggle/working/deberta_clickbait_tokenizer/vocab.json',\n '/kaggle/working/deberta_clickbait_tokenizer/merges.txt',\n '/kaggle/working/deberta_clickbait_tokenizer/added_tokens.json')"},"metadata":{}}]}]}